Решение задачи с помощью Data Science:
  1 - по каждой "новелле" собрать таблицы с общими данными, например, жанр, кол-во "основных персонажей", их имена,
  кол-во вопросов с выбором, длина новеллы(по кол-ву слов, например или же по категории: длинная, короткая и т.п.) и прочее(чем больше, тем лучше).
  2 - собрать в табличные данные информацию с действиями пользователя при взаимодействии с  вопросом в диалоге, где есть "платный выбор", а то есть:
  id пользователя, какая-то базовая информация, если есть(возраст, имя, страна и пр.), текст вопроса, есть ли "платный ответ", текст ответа №1, №2, №3,
  текст "платного" ответа(если он есть), имя персонажа с которым происходит взаимодействие и, непосредственно,
  был ли выбран вариант 1, 2, 3 или был выбран платный
  3 - привести две таблицы к общей форме, я это вижу как-то так:
  https://github.com/Anurion/HolyWater-test/issues/1#issue-1222825577
  соответственно, на этом этапе, чем больше категорий, тем лучше
  4 - провести лемматизацию с текстовыми данными(текст вопроса, например)
  5 - провести чистку данных: избавиться от неприменяемых строк, где образовалось много nulloв или же придумать, как их заполнить(например если утрачен
  возраст клиента, то можно взять моду(mode)
  6 - первичное обучение модели(что-то максимально простое,, чтобы понять feature importance и избавиться от "шума" (таргетом, соответственно является флаг
  "chose_paid", фичи с флагом, какой ответ был выбран, понятное дело, в обучении не участвуют)
  7 - обучение полноценной ML модели, которая в последствии пойдёт в прод. *это точно должно быть что-то "быстрое", нейронные сети строить не надо:)
  8 - теперь у нас есть модель, которая может проанализировать новую новеллу для всех наших клиентов, то есть:
    перед выпуском новой новеллы мы берём, например 10000 активных пользователей(в идеале profitable customers), к ним цепляем случайные вопросы из новой
	новеллы(в идеале как-то равномерно, чтобы не было вопросов, которые участвуют слишком мало)
  9 - на основании полученных результатов можно вносить изменения в построение новеллы для максимизации профита
  10 - прописать подробный пайплайн обучения модели и поставить её на регулярное обновление, основываясь на новых данных, получаемых с новых новелл и
  пользователей
  11 - наслаждаться результатом и следить, чтобы ничего не сломалось:)
  
  Технологии:
    GCP для первичного сбора и настройки потока данных(насколько я знаю, там можно это сделать)
    NLP(Lemmatization, nltk to remove stop-words, e.g. 'I', 'My', 'with' etc.)
    ML(одна из наиболее эффективных моделей: xgboost, lightgbm, catboost; GridSearchCV или для максимальной точности написать Bayesian CV,
	но ресурсов может потребоваться больше)
    GitLab, чтобы хранить на нём скрипт, для обучения модели
    Apache Airflow, чтобы регулярно брать скрипт с гитлаба и прогонять его, обучая модель заново на бОльших данных
      
  Ресурсы и затраты:
    Один DE, который сделает плавный поток необходимых данных во всех необходимых направлениях.
    Один DS(желательно с опытом в NLP), который обучит самую крутую модель, написав при этом грамотный код, который не придётся исправлять и "ускорять".
    Один Manager, который будет спрашивать у DS и DE как у них успехи:)
    Возможно DevOps, но в его необходимости я не уверен.
    DE должно хватить месяца на работу(2 недели до обучения модели, чтобы было из чего обучать и 2 после, чтобы полученные результаты не исчезали бесследно)
    DS думаю справится за 2 месяца, включая подготовку данных, обучение модели, подготовку DAGа и его установку на Airflow и остального необходимого.
    Модель должна на чём-то обучаться, данные должны где-то лежать и пр. думаю GCP всё это может предоставить. Непосредственно по цифрам я сориентировать,
    к сожалению, не могу.
    В сумме проект должен быть сделан и выведен в прод за 4 месяца, если брать один месяц для подстраховки и отчётности.
    
